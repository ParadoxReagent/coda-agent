# ── Required ──────────────────────────────────────
llm:
  default_provider: "anthropic"
  default_model: "claude-sonnet-4-5-20250514"
  providers:
    anthropic:
      type: "anthropic"
      api_key: ""  # or set ANTHROPIC_API_KEY env var
      models:
        - "claude-sonnet-4-5-20250514"
        - "claude-haiku-3-5-20241022"
  # Tier routing — use a cheaper model for simple requests, expensive for complex ones
  tiers:
    enabled: false
    light:
      provider: "anthropic"
      model: "claude-haiku-3-5-20241022"
    heavy:
      provider: "anthropic"
      model: "claude-sonnet-4-5-20250514"

discord:
  bot_token: ""       # or DISCORD_BOT_TOKEN
  channel_id: ""      # or DISCORD_CHANNEL_ID
  allowed_user_ids: [] # or DISCORD_ALLOWED_USER_IDS (comma-separated)

# ── Defaults (override if needed) ─────────────────
redis:
  url: "redis://localhost:6379"

database:
  url: "postgresql://localhost:5432/coda"

server:
  port: 3000
  host: "127.0.0.1"

skills:
  external_dirs: []
  agent_skill_dirs: []

# ── Enabled by default (zero-config) ─────────────
weather: {}         # Free NWS API, no key needed
# doctor runs automatically — see integrations_readme.md for tuning

# ── Optional Integrations ────────────────────────
# Each section below is OFF unless configured.
# See integrations_readme.md and skills_readme.md for full docs.
#
# Env var quick reference (auto-scaffolds providers when set):
#   ANTHROPIC_API_KEY, GOOGLE_API_KEY, OPENAI_API_KEY, OPENROUTER_API_KEY
#   FIRECRAWL_API_KEY        → web scraping/search
#   MEMORY_API_KEY           → semantic memory
#   EXECUTION_ENABLED=true   → Docker code execution
#   SLACK_APP_TOKEN + SLACK_BOT_TOKEN → Slack interface
#
# Uncomment sections below to customize defaults:

# google:
#   type: "google"
#   api_key: ""  # or GOOGLE_API_KEY
#   models:
#     - "gemini-2.0-flash"

# openai:
#   type: "openai_compat"
#   base_url: "https://api.openai.com/v1"
#   api_key: ""  # or OPENAI_API_KEY
#   models:
#     - "gpt-4o"

# firecrawl:
#   api_key: ""  # or FIRECRAWL_API_KEY (required for cloud)
#   # api_url: "http://localhost:3002"  # for self-hosted instance

# memory:
#   api_key: ""  # or MEMORY_API_KEY
#   base_url: "http://memory-service:8010"

# execution:
#   enabled: false  # or EXECUTION_ENABLED=true
#   default_image: "python:3.12-slim"
#   timeout: 60
#   max_memory: "512m"
#   network_enabled: false
#   allowed_images:  # Glob patterns for allowed Docker images
#     - "python:*"
#     - "node:*"
#     - "ubuntu:*"
#     - "alpine:*"
#     - "coda-skill-*"  # Pre-built skill images (use npm run build:skill-images)

# browser:
#   enabled: false  # or BROWSER_ENABLED=true
#   #
#   # Connection mode:
#   #   docker — spawn an isolated Docker container per session (default, production)
#   #            Build image first: docker compose --profile mcp-build build
#   #   host   — launch Chromium directly on the host (development/testing only)
#   #            Requires: npx playwright install chromium
#   mode: "docker"                          # "docker" or "host"
#   image: "coda-browser-sandbox"          # or BROWSER_IMAGE (docker mode only)
#   sandbox_network: "coda-browser-sandbox" # Internet-only network (docker mode only)
#   max_sessions: 3                         # Max concurrent browser sessions
#   session_timeout_seconds: 300            # Auto-destroy sessions idle > this (seconds)
#   tool_timeout_ms: 30000                  # Per-operation timeout (navigate, click, etc.)
#   connect_timeout_ms: 15000              # WebSocket connection timeout per attempt (docker mode)
#   connect_retries: 3                     # Retry attempts when connecting to container (docker mode)
#   headless: true                         # Run headless (host mode only; docker is always headless)
#   # URL allowlist — if non-empty, only these domains are reachable
#   url_allowlist: []
#   # Additional URL blocklist (private IPs/localhost are always blocked)
#   url_blocklist: []

# n8n:
#   webhooks:
#     # Auto-approved webhook — called without confirmation prompt (read-only operations)
#     web_research:
#       url: "https://n8n.example.com/webhook/your-webhook-id"
#       timeout_ms: 30000
#       description: "Web research — searches the web and returns summarized results"
#       requires_confirmation: false   # ← enables n8n_call_webhook (no prompt)
#     #
#     # Confirmation-required webhook (default when field is omitted)
#     send_email:
#       url: "https://n8n.example.com/webhook/another-webhook-id"
#       timeout_ms: 10000
#       description: "Send an email via n8n"
#       # requires_confirmation: true  ← default; use n8n_trigger_webhook
#     #
#     # Webhook with authentication
#     private_workflow:
#       url: "https://n8n.example.com/webhook/private-id"
#       timeout_ms: 15000
#       description: "Internal workflow requiring API key"
#       auth:
#         type: header
#         name: "X-Api-Key"
#         value: "your-secret-key"  # or use ${ENV_VAR} substitution

# slack:
#   app_token: ""  # or SLACK_APP_TOKEN
#   bot_token: ""  # or SLACK_BOT_TOKEN
#   channel_id: ""
#   allowed_user_ids: []

# telegram:
#   bot_token: ""       # or TELEGRAM_BOT_TOKEN (get from @BotFather)
#   chat_id: ""         # or TELEGRAM_CHAT_ID (numeric chat ID)
#   allowed_user_ids: [] # or TELEGRAM_ALLOWED_USER_IDS (comma-separated numeric user IDs)

# subagents:
#   enabled: true
#   default_timeout_minutes: 5
#   max_concurrent_per_user: 3

# doctor:
#   enabled: true
#   pattern_window_seconds: 300
#   pattern_threshold: 10
#   output_repair:
#     enabled: true
#     quick_fix_only: true  # SECURITY: true = no LLM repair (prevents injection)

# ── Self-Improvement Engine (Phase 4) ────────────
# Weekly Opus reflection, self-scoring, prompt evolution, learned routing.
# All sub-features are opt-in. Requires audit data to accumulate first.
#
# self_improvement:
#   enabled: true
#   opus_model: "claude-opus-4-6"      # Optional: use specific Opus model for reflection
#   reflection_cron: "0 3 * * 0"       # Sunday 3 AM (weekly Opus reflection)
#   assessment_enabled: true           # Post-turn self-scoring via Haiku
#   prompt_evolution_enabled: false    # Auto-apply approved prompt proposals (opt-in)
#   max_reflection_input_tokens: 8000  # Max tokens sent to Opus for reflection
#   approval_channel: "discord"        # Channel to deliver proposal summaries
#   routing_retrain_cron: "0 4 * * 0"  # Sunday 4 AM (learned classifier retraining)
#   # 5.3 Critique Loop — Haiku safety review before high-tier tool calls
#   critique_enabled: true             # Enable pre-execution critique for tier >= critique_min_tier
#   critique_min_tier: 3               # Minimum permission tier to trigger critique
#   # 5.4 Gap Detection — Monthly Opus analysis of capability gaps
#   gap_detection_enabled: true        # Enable monthly capability gap detection
#   gap_detection_cron: "0 2 1 * *"   # 1st of month 2 AM
#   # 5.7 Few-Shot Library — Harvest and inject solution patterns
#   few_shot_enabled: true             # Enable few-shot pattern harvesting and injection
#   few_shot_harvest_cron: "0 4 1 * *" # 1st of month 4 AM
#   few_shot_min_score: 4              # Min self-assessment score to qualify for harvest (0-5)
#   few_shot_min_tool_calls: 2         # Min tool calls in interaction to qualify

# ── Specialist Agents (Phase 5.2) ────────────────
# Per-agent overrides for directory-based specialists (home, research, lab, planner, ...).
# Specialist agents are defined as directories in src/agents/{name}/ containing:
#   soul.md       — system prompt (plain markdown)
#   tools.md      — allowed tools (one per line, # comments ok)
#   config.yaml   — description, model, token_budget, etc.
#   references/   — optional .md/.txt/.json docs appended to the system prompt
#
# All agents work with zero configuration. The specialists: section below allows
# per-agent overrides without editing agent directory files.
#
# specialists:
#   research:
#     system_prompt: "Custom research specialist prompt..."  # Override system prompt
#     allowed_tools:                      # Override allowed tools
#       - firecrawl_scrape
#       - firecrawl_search
#       - note_save
#     token_budget: 60000                # Override token budget
#   lab:
#     default_model: "claude-opus-4-6"   # Use a specific model for this specialist
#     default_provider: "anthropic"
#     max_tool_calls: 50                 # Override max tool calls

# ── Long-Horizon Tasks (Phase 4.5) ───────────────
# Persistent multi-day task tracking with checkpointing and auto-resumption.
#
# tasks:
#   enabled: true
#   resume_cron: "*/15 * * * *"        # Check for resumable tasks every 15 min
#   max_active_per_user: 5             # Max concurrent active tasks per user
#   max_auto_resume_attempts: 3        # Max auto-resumes without user interaction

# ── Alert Routing ────────────────────────────────
# Routes alert.* events from the event bus to Discord/Slack.
# Without rules, alerts are silently dropped (see AlertRouter.routeAlert).
#
# Rule properties:
#   severity: high | medium | low - minimum severity to route
#   channels: ["discord", "slack"] - where to send the alert
#   quietHours: true | false - whether quiet hours can suppress this alert
#   cooldown: number (seconds) - per-type cooldown to prevent spam (0 = no cooldown)
#
alerts:
  rules:
    # User-facing alerts
    "alert.reminder.due":
      severity: medium
      channels: ["discord"]
      quietHours: true      # Can be suppressed during quiet hours
      cooldown: 0           # No cooldown - each reminder fires once

    # System/operational alerts
    "alert.system.error":
      severity: high
      channels: ["discord"]
      quietHours: false     # Always deliver, even during quiet hours
      cooldown: 300         # 5 min cooldown to prevent error spam
    "alert.system.llm_failure":
      severity: high
      channels: ["discord"]
      quietHours: false
      cooldown: 600         # 10 min cooldown - circuit breaker opened
    "alert.system.llm_cost":
      severity: medium
      channels: ["discord"]
      quietHours: true
      cooldown: 3600        # 1 hour cooldown - daily cost threshold
    "alert.system.task_failed":
      severity: high
      channels: ["discord"]
      quietHours: false
      cooldown: 300         # 5 min cooldown - scheduled task failed
    "alert.system.dead_letter":
      severity: high
      channels: ["discord"]
      quietHours: false
      cooldown: 300         # 5 min cooldown - event handler failed 3x
    "alert.system.abuse":
      severity: high
      channels: ["discord"]
      quietHours: false
      cooldown: 600         # 10 min cooldown - abuse detection triggered

  # Quiet Hours Configuration
  # Suppress alerts during specified hours (except override_severities)
  quiet_hours:
    enabled: false                    # Set to true to enable quiet hours
    start: "22:00"                    # Start time (HH:MM)
    end: "07:00"                      # End time (HH:MM)
    timezone: "America/New_York"      # Timezone for quiet hours
    override_severities: ["high"]     # Severities that bypass quiet hours

# ── Audit & Observability ────────────────────────
# All tool calls are automatically logged to the audit_log table.
# No configuration required — always on. The agent can query its own
# audit history via the audit_query and audit_stats tools.
#
# Routing decisions are logged to routing_decisions table automatically
# when tier routing is enabled (TIER_ENABLED=true).

# ── Message Sender (Proactive Outbound) ──────────
# Controls how proactive messages are rate-limited.
# Skills use ctx.messageSender.send(channelId, message) to send proactively.
#
# message_sender:
#   rate_limit: 10  # Max messages per channel per hour (default: 10)

# ── MCP (Model Context Protocol) Integration
# Connect to external MCP servers and use their tools as coda skills
# See docs/mcp-integration.md for complete documentation
#
# mcp:
#   servers:
#     # Example 1: PDF processing (Python MCP server)
#     # Built-in server for PDF operations (merge, split, extract, rotate)
#     pdf:
#       enabled: true
#       transport:
#         type: stdio
#         command: python3
#         args: ["src/integrations/mcp/servers/pdf/server.py"]
#       description: "PDF processing tools (merge, split, extract text/tables, rotate)"
#       tool_timeout_ms: 120000  # 2 minutes for large PDFs
#       max_response_size: 500000  # 500KB for extracted text/tables
#
#     # Context7 - Live documentation and code examples (Docker-based)
#     # Build image first: npm run build:mcp-images
#     context7:
#       enabled: true
#       transport:
#         type: stdio
#         command: docker
#         args: ["run", "-i", "--rm", "coda-mcp-context7"]
#       description: "Context7 - Up-to-date documentation and code examples for libraries"
#       tool_timeout_ms: 60000
#       startup_mode: lazy  # eager (default) or lazy - lazy waits for first use
#       idle_timeout_minutes: 30  # Auto-disconnect after 30min idle (optional)
#
#     # Example 2: Local filesystem access via stdio
#     # Runs an MCP server as a child process
#     filesystem:
#       enabled: true
#       transport:
#         type: stdio
#         command: npx
#         args: ["-y", "@modelcontextprotocol/server-filesystem", "/allowed/directory"]
#         env:  # Optional environment variables
#           DEBUG: "mcp:*"
#         cwd: /working/directory  # Optional working directory
#       tool_blocklist: ["write_file", "delete_file"]  # Block dangerous operations
#       description: "Read-only filesystem access"
#       timeout_ms: 30000        # Connection timeout
#       tool_timeout_ms: 60000   # Per-tool execution timeout
#       max_response_size: 100000  # Max response size in bytes (100KB)
#
#     # Example 3: Remote MCP service via HTTP
#     # For production deployments, microservices, or external APIs
#     github:
#       enabled: false
#       transport:
#         type: http
#         url: https://mcp-github.example.com  # Remote URL
#         # Or use environment variables (recommended for secrets):
#         # url: ${MCP_GITHUB_URL}
#         headers:
#           Authorization: "Bearer ${GITHUB_TOKEN}"  # From env: GITHUB_TOKEN
#           X-Custom-Header: "value"
#       requires_confirmation: ["create_issue", "create_pull_request"]
#       sensitive_tools: ["get_token"]  # Logged at info level
#       timeout_ms: 60000
#
#     # Example 4: Docker container (same network)
#     # When both coda-agent and MCP server are in Docker
#     docker_service:
#       enabled: false
#       transport:
#         type: http
#         url: http://mcp-filesystem-container:8080  # Container name
#         # Or Docker Compose service name:
#         # url: http://mcp-service:8080
#       description: "Containerized MCP filesystem"
#
#     # Example 5: Host machine (when coda-agent is in Docker)
#     host_service:
#       enabled: false
#       transport:
#         type: http
#         # Mac/Windows:
#         url: http://host.docker.internal:8080
#         # Linux (requires --add-host=host-gateway:host-gateway):
#         # url: http://host-gateway:8080
#
#     # Example 6: Tool allowlist (strict mode)
#     # Only specified tools are available
#     database:
#       enabled: false
#       transport:
#         type: http
#         url: ${MCP_DATABASE_URL}
#       tool_allowlist:  # Allowlist takes precedence over blocklist
#         - query_read_only
#         - list_tables
#         - get_schema
#       # tool_blocklist is ignored when tool_allowlist is set
#       requires_confirmation: ["execute_migration"]
#       sensitive_tools: ["query_read_only"]
#
# Environment Variable Substitution:
# All string values support ${VAR_NAME} syntax for secrets.
#
# Method 1: .env file (recommended for local development)
#   Add to .env in project root:
#     MCP_GITHUB_URL=https://mcp-github.example.com
#     GITHUB_TOKEN=ghp_xxxxxxxxxxxx
#     MCP_DATABASE_URL=http://mcp-db:8080
#
# Method 2: Export directly (useful for Docker/CI)
#   export GITHUB_TOKEN="ghp_xxxxxxxxxxxx"
#   export MCP_GITHUB_URL="https://mcp-github.example.com"
#
# Security Notes:
# - Always use HTTPS for remote URLs (except localhost/internal networks)
# - Use environment variables for secrets (never commit tokens)
# - Set tool_blocklist to prevent dangerous operations
# - Mark sensitive tools with sensitive_tools
# - Use requires_confirmation for destructive actions
# - Set appropriate max_response_size to prevent memory issues

# Phase 7: Advanced Features — iOS App, Voice, Multi-Agent, Local LLM

**Timeline:** Long-term
**Depends on:** Phases 1-6 (stable, feature-rich platform)
**Goal:** Expand coda's reach with a native mobile app, voice interface, autonomous research capabilities, and resilience through local LLM fallback.

---

## 7.1 iOS App via REST API

### REST API Endpoint (`src/interfaces/rest-api.ts`)
- [ ] Implement Fastify REST API for custom clients:
  - `POST /api/v1/message` — send a message, get a response
  - `GET /api/v1/conversations` — list recent conversations
  - `GET /api/v1/briefing` — get current prepared briefing
  - `POST /api/v1/alerts/ack` — acknowledge an alert
  - `GET /api/v1/skills` — list available skills and their status
  - `WS /api/v1/stream` — WebSocket for real-time streaming responses and push alerts

### Authentication & Security
- [ ] **Tailscale-only exposure** — REST API binds to Tailscale interface, never public internet
- [ ] Authenticate via Tailscale identity headers (`Tailscale-User-Login`, `Tailscale-User-Name`)
- [ ] Fallback: JWT tokens issued via a one-time pairing flow:
  1. Generate pairing code shown on server logs/Discord
  2. Enter code in iOS app → receive JWT
  3. JWT includes userId, expires in 30 days, refresh token for rotation
- [ ] All endpoints require authentication
- [ ] Rate limiting per client (token bucket in Redis)

### iOS App (Native Swift)
- [ ] SwiftUI-based chat interface:
  - Message input with send button
  - Response rendering with markdown support
  - Streaming response display (via WebSocket)
- [ ] Push notifications via APNs:
  - coda REST API sends push via APNs when alerts fire
  - Notification categories: security (critical), info (normal), briefing (morning)
  - Critical alerts bypass Do Not Disturb on iOS
- [ ] Briefing widget (iOS WidgetKit):
  - Home screen widget showing today's summary
  - Tap to open full briefing in app
- [ ] Shortcuts integration:
  - Siri Shortcut: "Hey Siri, ask coda [question]"
  - Shortcuts app actions for common queries
- [ ] Offline handling: queue messages when tailnet is unreachable, send when reconnected

### API Versioning
- [ ] Version all endpoints under `/api/v1/`
- [ ] Breaking changes get a new version (`/api/v2/`)
- [ ] Document API with OpenAPI/Swagger (auto-generated by Fastify)

---

## 7.2 Voice Interface

### Speech-to-Text (STT)
- [ ] **Local Whisper** (primary): run OpenAI Whisper model locally for privacy
  - Use `whisper.cpp` with Node bindings (`whisper-node` or `@nicepkg/whisper-node`) for performance
  - Model size: `medium` for good accuracy/speed balance (or `large-v3` if GPU available)
  - Process audio in chunks for near-real-time transcription
- [ ] **Whisper API** (fallback): use OpenAI Whisper API if local resources insufficient
- [ ] Input sources:
  - iOS app: record audio, send as attachment via REST API
  - Discord: voice channel integration (complex, defer if needed)
  - Dedicated hardware: USB microphone on host with wake word detection

### Text-to-Speech (TTS)
- [ ] **Local TTS** (primary): use Piper TTS via subprocess or WASM for local synthesis
  - Low-latency, runs on CPU
  - Multiple voice options
- [ ] **Cloud TTS** (optional): ElevenLabs or Google Cloud TTS for higher quality
- [ ] Output:
  - iOS app: play audio response inline
  - Discord: send as audio file or join voice channel
  - Dedicated speaker: output to local audio device

### Wake Word Detection (Optional, Hardware Mode)
- [ ] Use `openWakeWord` or `Porcupine` for local wake word detection
- [ ] Custom wake word: "Hey coda"
- [ ] Flow: wake word → start recording → silence detection → STT → orchestrator → TTS → play response
- [ ] Hardware: Raspberry Pi with USB mic + speaker, connected to coda via REST API

### Voice Pipeline Integration
- [ ] Voice messages follow the same `orchestrator.handleMessage()` path
- [ ] Add `inputType: "voice"` metadata to conversation context
- [ ] LLM system prompt hint: "This message was spoken. Keep responses concise and conversational."
- [ ] Response length guidance: voice responses should be shorter than text responses

---

## 7.3 Multi-Agent Architecture

### Why Multi-Agent
Some tasks are too complex for a single orchestrator turn — research tasks, multi-step workflows, tasks requiring parallel information gathering. A multi-agent approach lets coda spawn specialized sub-agents.

### Agent Types
- [ ] **Research Agent**: autonomous web/database research
  - Given a topic, gathers information from multiple sources
  - Returns a structured report
  - Use case: "Research the latest on [CVE/topic/product]"
- [ ] **Workflow Agent**: multi-step task execution
  - Chains multiple skill calls in sequence with decision points
  - Use case: "Check if I have any meetings tomorrow and if not, schedule a movie night"
- [ ] **Analysis Agent**: data analysis and summarization
  - Processes large datasets from skills (network logs, email archives)
  - Use case: "Analyze my network traffic patterns over the last week"

### Implementation
- [ ] `AgentSpawner` class in orchestrator:
  - Main orchestrator decides when to spawn a sub-agent
  - Sub-agents get a scoped subset of tools and a focused system prompt
  - Sub-agents report back to the main orchestrator
  - Max concurrent sub-agents: 3 (configurable)
  - Sub-agent timeout: 5 minutes (configurable)
- [ ] Sub-agent isolation:
  - Separate conversation context (doesn't pollute main history)
  - Scoped tool access (research agent can't control Plex)
  - Token budget per sub-agent

### Agent-to-Agent Communication
- [ ] Sub-agents communicate results back via structured response format
- [ ] Main orchestrator synthesizes sub-agent results into user response
- [ ] Event bus used for async agent coordination:
  - `agent.task.started`, `agent.task.progress`, `agent.task.completed`

### User Visibility
- [ ] Show sub-agent status in Discord/Slack: "Researching... (checking 3 sources)"
- [ ] Progress updates for long-running tasks
- [ ] User can cancel sub-agent tasks

---

## 7.4 Proactive Research (Autonomous Tasks)

### Concept
Expand beyond reactive Q&A to autonomous background research. coda periodically performs tasks without being asked, surfacing findings when relevant.

### Use Cases
- [ ] **Security monitoring**: check for new CVEs affecting your stack
  - Periodically query NVD/CVE databases for products in your inventory
  - Alert when high-severity CVE matches your tech stack
- [ ] **Content discovery**: find content matching your interests
  - Monitor configured RSS feeds, Hacker News, Reddit
  - Score and surface relevant articles during briefing
- [ ] **Infrastructure forecasting**: predict capacity issues
  - Analyze trends in NAS storage usage, Proxmox resource consumption
  - "At current growth rate, your NAS will be full in 45 days"

### Guardrails
- [ ] All autonomous tasks are explicitly configured (no surprise actions)
- [ ] Read-only: autonomous tasks can gather and report but never take actions
- [ ] Resource budget: max LLM tokens per day for autonomous tasks (separate from interactive)
- [ ] Results queued for next briefing or interactive session, not pushed as alerts

---

## 7.5 Local LLM Fallback

### Purpose
If all cloud LLM providers are unavailable (outage, rate limit, network issue), fall back to a local LLM for basic functionality. This builds on Phase 1's provider abstraction and Phase 4's failover chain.

### How It Fits the Existing Architecture
The provider abstraction from Phase 1 already supports Ollama as a first-class provider (via `OpenAICompatProvider` with `baseURL: "http://localhost:11434/v1"`). Phase 4 adds per-provider circuit breakers and a failover chain. This section extends that with:
- Automatic fallback specifically to a **local** provider as the last resort
- Graceful degradation of features when running on a smaller model
- Health monitoring and auto-recovery

### Implementation
- [ ] Configure Ollama as the last provider in the Phase 4 failover chain:
  ```yaml
  llm:
    failover_chain: ["anthropic", "openai", "ollama"]
    providers:
      ollama:
        type: "openai_compat"
        base_url: "http://localhost:11434/v1"
        api_key: "ollama"
        models: ["llama3.1:8b", "mistral:7b"]
        local: true  # Flag indicating this is a local provider (always available)
  ```
- [ ] Model: `llama3.1:8b` or `mistral:7b` (balance of quality and resource usage)
- [ ] Fallback capabilities (reduced feature set):
  - Basic Q&A and conversation
  - Simple tool routing (direct skill calls)
  - Morning briefing (pre-formatted, minimal LLM reasoning)
- [ ] Limitations clearly communicated:
  - "Running in local mode — responses may be less detailed"
  - Complex multi-tool reasoning disabled
  - No sub-agent spawning

### Tool Compatibility
- [ ] Ollama's OpenAI-compatible endpoint supports function calling for some models
- [ ] For models without native tool support, implement a tool-use shim in `OpenAICompatProvider`:
  - Convert tool definitions to a prompt-based format the local model understands
  - Parse structured output from local model to extract tool calls
  - Fallback to regex extraction if structured output fails
- [ ] Test each skill with local LLM and document which work reliably

### Auto-Recovery
- [ ] Phase 4's circuit breaker already handles per-provider health monitoring
- [ ] Background health check on cloud providers every 60 seconds during local fallback
- [ ] Auto-switch back to primary provider when it recovers
- [ ] Notify user: "Primary provider recovered — back to full capability"

---

## 7.6 Test Suite — Phase 7 Gate

All tests must pass. Run with `npm run test:phase7`.

### Unit Tests

**REST API (`tests/unit/interfaces/rest-api.test.ts`)**
- [ ] `POST /api/v1/message` accepts message and returns response
- [ ] `GET /api/v1/briefing` returns prepared briefing
- [ ] `GET /api/v1/skills` returns skill status list
- [ ] Unauthenticated requests return 401
- [ ] Invalid JWT returns 401
- [ ] Expired JWT returns 401 with refresh hint
- [ ] Rate limiting returns 429 after threshold
- [ ] WebSocket connection receives streamed responses
- [ ] WebSocket connection receives push alerts

**JWT Auth (`tests/unit/interfaces/auth.test.ts`)**
- [ ] Pairing code generation produces unique codes
- [ ] Valid pairing code exchanges for JWT + refresh token
- [ ] JWT contains correct user claims and expiry
- [ ] Refresh token rotates JWT successfully
- [ ] Used pairing codes are invalidated
- [ ] Tailscale identity headers bypass JWT requirement

**Voice Pipeline (`tests/unit/core/voice.test.ts`)**
- [ ] Audio buffer is transcribed to text via Whisper
- [ ] Transcribed text flows through `orchestrator.handleMessage()`
- [ ] Voice metadata is attached to conversation context
- [ ] TTS converts response text to audio buffer
- [ ] Voice responses are shorter than equivalent text responses (system prompt hint)

**Multi-Agent (`tests/unit/core/agents.test.ts`)**
- [ ] `AgentSpawner` creates sub-agent with scoped tools
- [ ] Sub-agent cannot access tools outside its scope
- [ ] Sub-agent respects token budget
- [ ] Sub-agent respects timeout (5 minutes)
- [ ] Max concurrent sub-agents limit is enforced
- [ ] Sub-agent results are returned to main orchestrator
- [ ] Sub-agent failure is handled gracefully (doesn't crash orchestrator)
- [ ] User cancellation stops sub-agent execution

**Local LLM Fallback (`tests/unit/core/llm-fallback.test.ts`)**
- [ ] Fallback triggers after all cloud providers in failover chain are unavailable
- [ ] Fallback triggers on network connectivity loss to all cloud providers
- [ ] Fallback mode routes requests to local Ollama provider
- [ ] Tool-use shim converts tool definitions to prompt format for models without native support
- [ ] Tool-use shim parses tool calls from local model output
- [ ] Fallback mode communicates limitations to user
- [ ] Auto-recovery switches back when primary cloud provider returns healthy
- [ ] Recovery notification is sent to user

**Proactive Research (`tests/unit/core/research.test.ts`)**
- [ ] Configured research tasks run on schedule
- [ ] Research tasks respect daily token budget
- [ ] Research results are queued for briefing, not pushed as alerts
- [ ] Research tasks are read-only (no skill actions executed)
- [ ] Unconfigured research tasks do not run

### Integration Tests

**REST API E2E (`tests/integration/rest-api-e2e.test.ts`)**
- [ ] Full flow: authenticate → send message → receive response
- [ ] Full flow: authenticate → open WebSocket → receive streamed response
- [ ] Alert fires → WebSocket client receives push notification
- [ ] Pairing flow: generate code → exchange for JWT → use JWT for requests

**Multi-Agent Workflow (`tests/integration/multi-agent.test.ts`)**
- [ ] Research agent: given topic → gathers info from browser skill → returns report
- [ ] Workflow agent: checks calendar → makes decision → executes action
- [ ] Progress updates are sent to interface during long-running agent tasks
- [ ] User cancellation propagates and stops active agents

**LLM Failover (`tests/integration/llm-failover.test.ts`)**
- [ ] All cloud providers go down → system switches to local Ollama within 30s
- [ ] Local LLM handles basic queries and simple tool routing
- [ ] Primary cloud provider recovers → system switches back automatically
- [ ] In-flight messages during switchover are handled (not dropped)

**Voice E2E (`tests/integration/voice-e2e.test.ts`)**
- [ ] Audio file upload → STT → orchestrator → TTS → audio response returned
- [ ] iOS client sends voice attachment → receives audio response via REST API

### Test Helpers (additions to previous phases)
- [ ] `createMockOllamaServer()` — mock Ollama API with configurable model responses
- [ ] `createMockWhisperEngine()` — mock STT that returns configured transcriptions
- [ ] `createMockTTSEngine()` — mock TTS that returns audio buffers
- [ ] `createTestJWT()` — factory for valid/expired/invalid JWT tokens
- [ ] `createMockWebSocket()` — mock WebSocket client for testing push notifications

---

## Acceptance Criteria

1. iOS app connects via Tailscale and sends/receives messages through the REST API
2. Push notifications arrive on iOS for alerts (security alerts bypass DND)
3. Voice input via iOS app is transcribed and processed as a normal message
4. TTS response plays back in the iOS app
5. "Research [topic]" spawns a sub-agent that gathers info and returns a report
6. Sub-agent progress is visible in the chat interface
7. If all cloud LLM providers go down, coda falls back to local Ollama within 30 seconds
8. Local LLM mode handles basic queries and skill routing (with reduced quality)
9. Auto-recovery switches back to primary cloud provider when it is restored
10. Autonomous security monitoring alerts on new CVEs matching the configured tech stack
11. **`npm run test:phase7` passes with 0 failures**

---

## Key Decisions for This Phase

| Decision | Choice | Rationale |
|----------|--------|-----------|
| iOS framework | SwiftUI + WidgetKit | Modern, native feel, widget support |
| API auth | Tailscale identity + JWT fallback | Zero-trust networking, no public exposure |
| STT engine | whisper.cpp with Node bindings (local) | Privacy, no cloud dependency, good perf |
| TTS engine | Piper TTS via subprocess (local) | Low-latency, CPU-friendly, open source |
| Local LLM runtime | Ollama (via existing provider abstraction) | Already a configured provider from Phase 1, no new code needed |
| Local LLM model | llama3.1:8b | Best balance of quality and resource usage |
| Sub-agent limit | 3 concurrent, 5 min timeout | Resource control, prevent runaway agents |
| Autonomous task scope | Read-only, budgeted | Safety — autonomous actions are dangerous |
| WebSocket | Fastify WebSocket plugin | Same server, no separate WS process |

---

## Key Dependencies (additions to previous phases)

```json
{
  "dependencies": {
    "@fastify/websocket": "^11.0.0",
    "jose": "^6.0.0"
  }
}
```

Note: Voice (Whisper, Piper TTS) integrations run as subprocesses or via WASM — managed by the voice skill, not as npm dependencies. Ollama is a standalone service accessed via its REST API using native `fetch`. The iOS app is a separate Swift project.

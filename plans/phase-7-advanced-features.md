# Phase 7: Advanced Features — iOS App, Voice, Multi-Agent, Local LLM

**Timeline:** Long-term
**Depends on:** Phases 1-6 (stable, feature-rich platform)
**Goal:** Expand coda's reach with a native mobile app, voice interface, autonomous research capabilities, and resilience through local LLM fallback.

---

## 7.1 iOS App via REST API

### REST API Endpoint (`src/interfaces/rest-api.ts`)
- [ ] Implement Fastify REST API for custom clients:
  - `POST /api/v1/message` — send a message, get a response
  - `GET /api/v1/conversations` — list recent conversations
  - `GET /api/v1/briefing` — get current prepared briefing
  - `POST /api/v1/alerts/ack` — acknowledge an alert
  - `GET /api/v1/skills` — list available skills and their status
  - `WS /api/v1/stream` — WebSocket for real-time streaming responses and push alerts

### Authentication & Security
- [ ] **Tailscale-only exposure** — REST API binds to Tailscale interface, never public internet
- [ ] Authenticate via Tailscale identity headers (`Tailscale-User-Login`, `Tailscale-User-Name`) only when injected by a trusted local component (`tsnet` / Tailscale Serve)
  - Reject direct client-supplied identity headers unless request source is verified trusted
  - Strip and ignore forwarding identity headers at edge boundaries by default
- [ ] Fallback: JWT tokens issued via a one-time pairing flow:
  1. Generate pairing code shown only in trusted operator channels
  2. Pairing code is short-lived (default 5 minutes), single-use, and attempt-limited
  3. Enter code in iOS app with device proof (ephemeral public key / nonce) → receive JWT
  4. JWT includes userId and deviceId, expires in 30 days, refresh token for rotation
  5. Refresh token is stored server-side as a hash and can be revoked
- [ ] All endpoints require authentication
- [ ] Rate limiting per client (token bucket in Redis)
- [ ] JWT validation hardening:
  - Validate `iss`, `aud`, `exp`, `nbf`, and `iat` with bounded clock skew
  - Sign with rotating keys (`kid`) and maintain key rollover policy
  - Reject algorithm confusion (`alg`) and unsigned tokens
- [ ] WebSocket auth hardening:
  - Require auth at connection establishment and revalidate token on reconnect
  - Enforce Origin allowlist for browser-based clients

### iOS App (Native Swift)
- [ ] SwiftUI-based chat interface:
  - Message input with send button
  - Response rendering with markdown support
  - Streaming response display (via WebSocket)
- [ ] Push notifications via APNs:
  - coda REST API sends push via APNs when alerts fire
  - Notification categories: security (critical), info (normal), briefing (morning)
  - Critical alerts bypass Do Not Disturb on iOS
- [ ] Briefing widget (iOS WidgetKit):
  - Home screen widget showing today's summary
  - Tap to open full briefing in app
- [ ] Shortcuts integration:
  - Siri Shortcut: "Hey Siri, ask coda [question]"
  - Shortcuts app actions for common queries
- [ ] Offline handling: queue messages when tailnet is unreachable, send when reconnected

### API Versioning
- [ ] Version all endpoints under `/api/v1/`
- [ ] Breaking changes get a new version (`/api/v2/`)
- [ ] Document API with OpenAPI/Swagger (auto-generated by Fastify)

---

## 7.2 Voice Interface

### Speech-to-Text (STT)
- [ ] **Local Whisper** (primary): run OpenAI Whisper model locally for privacy
  - Use `whisper.cpp` with Node bindings (`whisper-node` or `@nicepkg/whisper-node`) for performance
  - Model size: `medium` for good accuracy/speed balance (or `large-v3` if GPU available)
  - Process audio in chunks for near-real-time transcription
- [ ] **Whisper API** (fallback): use OpenAI Whisper API if local resources insufficient
- [ ] Input sources:
  - iOS app: record audio, send as attachment via REST API
  - Discord: voice channel integration (complex, defer if needed)
  - Dedicated hardware: USB microphone on host with wake word detection

### Text-to-Speech (TTS)
- [ ] **Local TTS** (primary): use Piper TTS via subprocess or WASM for local synthesis
  - Low-latency, runs on CPU
  - Multiple voice options
- [ ] **Cloud TTS** (optional): ElevenLabs or Google Cloud TTS for higher quality
- [ ] Output:
  - iOS app: play audio response inline
  - Discord: send as audio file or join voice channel
  - Dedicated speaker: output to local audio device

### Wake Word Detection (Optional, Hardware Mode)
- [ ] Use `openWakeWord` or `Porcupine` for local wake word detection
- [ ] Custom wake word: "Hey coda"
- [ ] Flow: wake word → start recording → silence detection → STT → orchestrator → TTS → play response
- [ ] Hardware: Raspberry Pi with USB mic + speaker, connected to coda via REST API

### Voice Pipeline Integration
- [ ] Voice messages follow the same `orchestrator.handleMessage()` path
- [ ] Add `inputType: "voice"` metadata to conversation context
- [ ] LLM system prompt hint: "This message was spoken. Keep responses concise and conversational."
- [ ] Response length guidance: voice responses should be shorter than text responses

---

## 7.3 Multi-Agent Architecture — IMPLEMENTED

> **Status:** Implemented. See `plans/subagent-system.md` for full documentation.

The sub-agent system is live with both synchronous (`delegate_to_subagent`) and asynchronous (`sessions_spawn`) delegation modes, a `BaseAgent` abstraction, and comprehensive security controls.

### What Was Built
- [x] `BaseAgent` class (`src/core/base-agent.ts`) — unified agentic loop for both main agent and sub-agents
- [x] `SubagentManager` (`src/core/subagent-manager.ts`) — lifecycle management: spawn, track, timeout, cancel, cleanup
- [x] `SubagentSkill` (`src/skills/subagents/skill.ts`) — 7 LLM tools for delegation and management
- [x] `mainAgentOnly` flag on `SkillToolDefinition` — declarative tool scoping for sub-agents
- [x] Recursive spawn prevention (two layers: declarative + runtime guard)
- [x] User isolation, output sanitization, rate limiting, concurrency limits
- [x] Discord `/subagents` slash command (list, stop, log, info, send)
- [x] Event bus integration (spawned, running, completed, failed, timeout, cancelled)
- [x] Database schema for persistent audit trail (`subagent_runs` table)
- [x] Configurable limits (timeout, concurrency, token budget, rate limits)

### Original Concepts Addressed
- [x] **Research/Workflow/Analysis agents**: Handled generically — any task can be delegated with scoped tools. Named agent types deferred to a future Critic/Reviewer workflow.
- [x] **Sub-agent isolation**: Fresh `BaseAgent` instance per run with separate context and scoped tools.
- [x] **Agent-to-agent communication**: Results returned via sanitized output (sync) or announced via callback (async). Events published throughout lifecycle.
- [x] **User visibility**: `/subagents list` and `sessions_list` show status. `/subagents log` shows transcript. Async results announced to channel.
- [x] **Cancellation**: `/subagents stop` and `sessions_stop` abort running agents via `AbortController`.

---

## 7.4 Proactive Research (Autonomous Tasks)

### Concept
Expand beyond reactive Q&A to autonomous background research. coda periodically performs tasks without being asked, surfacing findings when relevant.

### Use Cases
- [ ] **Security monitoring**: check for new CVEs affecting your stack
  - Periodically query NVD/CVE databases for products in your inventory
  - Alert when high-severity CVE matches your tech stack
- [ ] **Content discovery**: find content matching your interests
  - Monitor configured RSS feeds, Hacker News, Reddit
  - Score and surface relevant articles during briefing
- [ ] **Infrastructure forecasting**: predict capacity issues
  - Analyze trends in NAS storage usage, Proxmox resource consumption
  - "At current growth rate, your NAS will be full in 45 days"

### Guardrails
- [ ] All autonomous tasks are explicitly configured (no surprise actions)
- [ ] Read-only: autonomous tasks can gather and report but never take actions
- [ ] Resource budget: max LLM tokens per day for autonomous tasks (separate from interactive)
- [ ] Results queued for next briefing or interactive session, not pushed as alerts by default
- [ ] Exception: high-severity security findings (e.g., matching critical CVEs) may emit immediate security alerts

---

## 7.5 Local LLM Fallback

### Purpose
If all cloud LLM providers are unavailable (outage, rate limit, network issue), fall back to a local LLM for basic functionality. This builds on Phase 1's provider abstraction and Phase 4's failover chain.

### How It Fits the Existing Architecture
The provider abstraction from Phase 1 already supports Ollama as a first-class provider (via `OpenAICompatProvider` with `baseURL: "http://localhost:11434/v1"`). Phase 4 adds per-provider circuit breakers and a failover chain. This section extends that with:
- Automatic fallback specifically to a **local** provider as the last resort
- Graceful degradation of features when running on a smaller model
- Health monitoring and auto-recovery

### Implementation
- [ ] Configure Ollama as the last provider in the Phase 4 failover chain:
  ```yaml
  llm:
    failover_chain: ["anthropic", "openai", "ollama"]
    providers:
      ollama:
        type: "openai_compat"
        base_url: "http://localhost:11434/v1"
        api_key: "ollama"
        models: ["llama3.1:8b", "mistral:7b"]
        local: true  # Flag indicating this is a local provider (always available)
  ```
- [ ] Model: `llama3.1:8b` or `mistral:7b` (balance of quality and resource usage)
- [ ] Fallback capabilities (reduced feature set):
  - Basic Q&A and conversation
  - Simple tool routing (direct skill calls)
  - Morning briefing (pre-formatted, minimal LLM reasoning)
- [ ] Limitations clearly communicated:
  - "Running in local mode — responses may be less detailed"
  - Complex multi-tool reasoning disabled
  - No sub-agent spawning

### Tool Compatibility
- [ ] Ollama's OpenAI-compatible endpoint supports function calling for some models
- [ ] For models without native tool support, implement a tool-use shim in `OpenAICompatProvider`:
  - Convert tool definitions to a prompt-based format the local model understands
  - Parse structured output from local model to extract tool calls
  - Fallback to regex extraction if structured output fails
- [ ] Test each skill with local LLM and document which work reliably

### Auto-Recovery
- [ ] Phase 4's circuit breaker already handles per-provider health monitoring
- [ ] Background health check on cloud providers every 60 seconds during local fallback
- [ ] Auto-switch back to primary provider when it recovers
- [ ] Notify user: "Primary provider recovered — back to full capability"

---

## 7.6 Test Suite — Phase 7 Gate

Gate-tier tests must pass. Run with `npm run test:phase7`.
- Gate: deterministic unit + integration tests (no live network dependency)
- Advisory: live-provider contract checks (non-blocking)
- Nightly: full end-to-end against real external services

### Unit Tests

**REST API (`tests/unit/interfaces/rest-api.test.ts`)**
- [ ] `POST /api/v1/message` accepts message and returns response
- [ ] `GET /api/v1/briefing` returns prepared briefing
- [ ] `GET /api/v1/skills` returns skill status list
- [ ] Unauthenticated requests return 401
- [ ] Invalid JWT returns 401
- [ ] Expired JWT returns 401 with refresh hint
- [ ] Rate limiting returns 429 after threshold
- [ ] WebSocket connection receives streamed responses
- [ ] WebSocket connection receives push alerts

**JWT Auth (`tests/unit/interfaces/auth.test.ts`)**
- [ ] Pairing code generation produces unique codes
- [ ] Valid pairing code exchanges for JWT + refresh token
- [ ] Pairing code expires after TTL and cannot be reused
- [ ] Pairing flow enforces attempt limits and lockout on repeated failures
- [ ] Pairing exchange requires valid device proof
- [ ] JWT contains correct user claims and expiry
- [ ] JWT includes `iss`/`aud` and rejects invalid issuer/audience
- [ ] JWT with invalid `alg` or missing signature is rejected
- [ ] JWT key rotation (`kid`) validates old/new keys during rollout
- [ ] Refresh token rotates JWT successfully
- [ ] Refresh tokens are stored hashed and revoked tokens are rejected
- [ ] Used pairing codes are invalidated
- [ ] Tailscale identity headers bypass JWT only for trusted local source paths
- [ ] Client-supplied spoofed identity headers are rejected

**Voice Pipeline (`tests/unit/core/voice.test.ts`)**
- [ ] Audio buffer is transcribed to text via Whisper
- [ ] Transcribed text flows through `orchestrator.handleMessage()`
- [ ] Voice metadata is attached to conversation context
- [ ] TTS converts response text to audio buffer
- [ ] Voice responses are shorter than equivalent text responses (system prompt hint)

**Multi-Agent — IMPLEMENTED (76+ tests across 5 files)**
- [x] `BaseAgent` unit tests (`tests/unit/core/base-agent.test.ts`, 13 tests)
- [x] `SubagentManager` unit tests (`tests/unit/core/subagent-manager.test.ts`, 28 tests)
- [x] `SubagentSkill` unit tests (`tests/unit/skills/subagents/skill.test.ts`, 17 tests)
- [x] Lifecycle integration tests (`tests/integration/subagent-lifecycle.test.ts`, 8 tests)
- [x] Security integration tests (`tests/integration/subagent-security.test.ts`, 10 tests)
- [x] Registry filtering tests (`tests/unit/skills/registry.test.ts`, +4 tests)
- [x] Sanitizer tests (`tests/unit/core/sanitizer.test.ts`, +3 tests)

**Local LLM Fallback (`tests/unit/core/llm-fallback.test.ts`)**
- [ ] Fallback triggers after all cloud providers in failover chain are unavailable
- [ ] Fallback triggers on network connectivity loss to all cloud providers
- [ ] Fallback mode routes requests to local Ollama provider
- [ ] Tool-use shim converts tool definitions to prompt format for models without native support
- [ ] Tool-use shim parses tool calls from local model output
- [ ] Fallback mode communicates limitations to user
- [ ] Auto-recovery switches back when primary cloud provider returns healthy
- [ ] Recovery notification is sent to user

**Proactive Research (`tests/unit/core/research.test.ts`)**
- [ ] Configured research tasks run on schedule
- [ ] Research tasks respect daily token budget
- [ ] Research results are queued for briefing, not pushed as alerts
- [ ] Research tasks are read-only (no skill actions executed)
- [ ] Unconfigured research tasks do not run

### Integration Tests

**REST API E2E (`tests/integration/rest-api-e2e.test.ts`)**
- [ ] Full flow: authenticate → send message → receive response
- [ ] Full flow: authenticate → open WebSocket → receive streamed response
- [ ] Alert fires → WebSocket client receives push notification
- [ ] Pairing flow: generate code → exchange for JWT → use JWT for requests

**Multi-Agent Workflow — IMPLEMENTED**
- [x] Async spawn + list + info lifecycle (`tests/integration/subagent-lifecycle.test.ts`)
- [x] Spawn + stop cancellation with event verification
- [x] Sync delegation returns sanitized result
- [x] Multiple concurrent spawns within limits
- [x] Tool registration and scoping via SubagentSkill
- [x] Announce callback on async completion
- [x] Events published throughout lifecycle
- [x] Recursive spawn prevention (`tests/integration/subagent-security.test.ts`)
- [x] User isolation (cross-user access denied)
- [x] Injection sanitization
- [x] Rate limiting enforcement
- [x] `mainAgentOnly` enforcement
- [x] Concurrency limit enforcement

**LLM Failover (`tests/integration/llm-failover.test.ts`)**
- [ ] All cloud providers go down → system switches to local Ollama within 30s
- [ ] Local LLM handles basic queries and simple tool routing
- [ ] Primary cloud provider recovers → system switches back automatically
- [ ] In-flight messages during switchover are handled (not dropped)

**Voice E2E (`tests/integration/voice-e2e.test.ts`)**
- [ ] Audio file upload → STT → orchestrator → TTS → audio response returned
- [ ] iOS client sends voice attachment → receives audio response via REST API

### Test Helpers (additions to previous phases)
- [ ] `createMockOllamaServer()` — mock Ollama API with configurable model responses
- [ ] `createMockWhisperEngine()` — mock STT that returns configured transcriptions
- [ ] `createMockTTSEngine()` — mock TTS that returns audio buffers
- [ ] `createTestJWT()` — factory for valid/expired/invalid JWT tokens
- [ ] `createMockWebSocket()` — mock WebSocket client for testing push notifications

---

## Acceptance Criteria

1. iOS app connects via Tailscale and sends/receives messages through the REST API
2. Push notifications arrive on iOS for alerts (security alerts bypass DND)
3. Voice input via iOS app is transcribed and processed as a normal message
4. TTS response plays back in the iOS app
5. "Research [topic]" spawns a sub-agent that gathers info and returns a report
6. Sub-agent progress is visible in the chat interface
7. If all cloud LLM providers go down, coda falls back to local Ollama within 30 seconds
8. Local LLM mode handles basic queries and skill routing (with reduced quality)
9. Auto-recovery switches back to primary cloud provider when it is restored
10. Autonomous security monitoring alerts on new CVEs matching the configured tech stack
11. **`npm run test:phase7` passes with 0 failures**

---

## Key Decisions for This Phase

| Decision | Choice | Rationale |
|----------|--------|-----------|
| iOS framework | SwiftUI + WidgetKit | Modern, native feel, widget support |
| API auth | Tailscale identity + JWT fallback | Zero-trust networking, no public exposure |
| STT engine | whisper.cpp with Node bindings (local) | Privacy, no cloud dependency, good perf |
| TTS engine | Piper TTS via subprocess (local) | Low-latency, CPU-friendly, open source |
| Local LLM runtime | Ollama (via existing provider abstraction) | Already a configured provider from Phase 1, no new code needed |
| Local LLM model | llama3.1:8b | Best balance of quality and resource usage |
| Sub-agent limit | 3 concurrent, 5 min timeout | Resource control, prevent runaway agents |
| Autonomous task scope | Read-only, budgeted | Safety — autonomous actions are dangerous |
| WebSocket | Fastify WebSocket plugin | Same server, no separate WS process |

---

## Key Dependencies (additions to previous phases)

```json
{
  "dependencies": {
    "@fastify/websocket": "^11.0.0",
    "jose": "^6.0.0"
  }
}
```

Note: Voice (Whisper, Piper TTS) integrations run as subprocesses or via WASM — managed by the voice skill, not as npm dependencies. Ollama is a standalone service accessed via its REST API using native `fetch`. The iOS app is a separate Swift project.
